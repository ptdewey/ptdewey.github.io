[
  {
    "metadata": {
      "authors": [
        "Patrick Dewey"
      ],
      "categories": [
        "Software Development"
      ],
      "date": "2024-11-26",
      "read_time": 5,
      "slug": "bluesky-comments-svelte",
      "tags": [
        "bluesky",
        "software-development",
        "svelte",
        "npm"
      ],
      "title": "Adding Comments to My Blog with Bluesky!"
    },
    "content": "\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eBluesky has been gaining a lot of traction and publicity recently, and there are a few aspects of the platform that have piqued my interest in a way that no other social media platform has done.\u003c/p\u003e\n\u003cp\u003eWhat gets me most excited about Bluesky is that both the \u003ca href=\"https://bsky.app\"\u003eplatform\u003c/a\u003e and the underlying \u003ca href=\"https://atproto.com/\"\u003eAT Protocol\u003c/a\u003e are open source\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e. The open source nature of the platform is great for transparency (something other platforms sorely lack), and also makes it really easy for developers to improve on and build off of the platform. (The AT protocol is also very interesting and will likely enable many exciting projects in the future, but that's a topic for another day)\u003c/p\u003e\n\u003cp\u003eWhat I'd like to talk about today is one specific example of a tool that was built off of Bluesky, embeddable blog post comments.\u003c/p\u003e\n\u003ch2\u003eEmbedding Bluesky for Blog Comments\u003c/h2\u003e\n\u003cp\u003eThe idea of using Bluesky threads as blog comments was first debuted by \u003ca href=\"https://bsky.app/profile/emilyliu.me\"\u003eEmily Liu\u003c/a\u003e (a Bluesky developer) in a \u003ca href=\"https://emilyliu.me/blog/open-network\"\u003epost\u003c/a\u003e a couple of days ago. It quickly got people very excited, with many requests for example code.\u003c/p\u003e\n\u003cp\u003eFortunately, she published her code in a new \u003ca href=\"https://bsky.app/profile/emilyliu.me/post/3lbqta5lnck2i\"\u003epost\u003c/a\u003e the very next day. Unfortunately, her code\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e was built specifically around NextJS and TailwindCSS, but this code was enough of a starting point for me and (presumably) a number of other developers to start adapting it for other frameworks and applications.\u003c/p\u003e\n\u003cp\u003eI found \u003ca href=\"https://bsky.app/profile/coryzue.com\"\u003eCory Zue's\u003c/a\u003e implementation\u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e in React pretty early on in its development, but I wasn't able to get it working with my site, as (at the time) the library only worked with React projects. As such, I set out to make my own version of the library in Svelte, since that is the framework I use for my site.\u003c/p\u003e\n\u003ch3\u003eBuilding the Library\u003c/h3\u003e\n\u003cp\u003eTo begin, I used the Svelte library creation tool (\u003ccode\u003enpx sv create\u003c/code\u003e) to set up a base project (I chose to use JavaScript and JSDoc for types).\u003c/p\u003e\n\u003cp\u003eIn a similar design to Emily Liu, I created Svelte components for 'Actions', 'Comments', and the 'CommentSection', with the 'CommentSection' being the main exported component to be used in other pages. I wrote the component in a way that allows for either a post URI or a Bluesky handle to be used, with the handle option using the Bluesky API to find the most recent post containing the blog post URL (returning an error message if none are found).\u003c/p\u003e\n\u003cp\u003eI set up some basic CSS styling for the components, and created some variables to allow styling of certain aspects to be outsourced to library users. This part of the library could likely be improved; I was unsure about which parts of the styling should be handled on the library's end instead of being delegated to users. I hope to address this soon (I'd also very happily accept \u003ca href=\"https://github.com/ptdewey/bluesky-comments-svelte/issues\"\u003efeature requests\u003c/a\u003e and \u003ca href=\"https://github.com/ptdewey/bluesky-comments-svelte/pulls\"\u003econtributions\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eThe last step was publishing the library so that I (and hopefully others) could use the library. I had never published an NPM package before, but the process was actually quite simple, as the library creation tool provided the necessary scripts. After running \u003ccode\u003enpm package\u003c/code\u003e (and making some tweaks to 'package.json'), my library was published to the NPM registry\u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e. (The \u003ca href=\"https://github.com/ptdewey/bluesky-comments-svelte\"\u003esource code\u003c/a\u003e is also available\u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e)\u003c/p\u003e\n\u003ch2\u003eIntegrating the Library Into My Site\u003c/h2\u003e\n\u003cp\u003eTo get the newly published library integrated with my site, all I had to do was install it:\u003c/p\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e1\u003c/span\u003e\u003cspan class=\"cl\"\u003enpm install bluesky-comments-svelte\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eImport the component into my blog post '+page.svelte' file and set the author prop:\u003c/p\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e1\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nt\"\u003escript\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e2\u003c/span\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"kr\"\u003eimport\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nx\"\u003eCommentSection\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"nx\"\u003efrom\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;bluesky-comments-svelte\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e3\u003c/span\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"kr\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003eauthor\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;pdewey.com\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e4\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e\u0026lt;/\u003c/span\u003e\u003cspan class=\"nt\"\u003escript\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAnd then add the component to the page itself:\u003c/p\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e1\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nt\"\u003ediv\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e2\u003c/span\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nt\"\u003eh2\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003eComments\u003cspan class=\"p\"\u003e\u0026lt;/\u003c/span\u003e\u003cspan class=\"nt\"\u003eh2\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e3\u003c/span\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nt\"\u003ediv\u003c/span\u003e \u003cspan class=\"na\"\u003eclass\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;comment-section\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e4\u003c/span\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nt\"\u003eCommentSection\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nx\"\u003eauthor\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"na\"\u003eopts\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{{\u003c/span\u003e \u003cspan class=\"nx\"\u003eshowCommentsTitle\u003c/span\u003e: \u003cspan class=\"kt\"\u003efalse\u003c/span\u003e \u003cspan class=\"p\"\u003e}}\u003c/span\u003e \u003cspan class=\"s\"\u003e/\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e5\u003c/span\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e\u0026lt;/\u003c/span\u003e\u003cspan class=\"nt\"\u003ediv\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e6\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e\u0026lt;/\u003c/span\u003e\u003cspan class=\"nt\"\u003ediv\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eWith just a few styling modifications (mostly colors), everything looked the way I wanted it.\u003c/p\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e 1\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nt\"\u003estyle\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e 2\u003c/span\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"nd\"\u003eroot\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e 3\u003c/span\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nv\"\u003e--comment-border-color\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nf\"\u003evar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e--\u003c/span\u003e\u003cspan class=\"kc\"\u003etan\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e 4\u003c/span\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nv\"\u003e--avatar-size\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"kt\"\u003erem\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e 5\u003c/span\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nv\"\u003e--font-size-title\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e1.5\u003c/span\u003e\u003cspan class=\"kt\"\u003erem\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e 6\u003c/span\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nv\"\u003e--handle-color\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nf\"\u003evar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e--\u003c/span\u003e\u003cspan class=\"kc\"\u003egreen\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e 7\u003c/span\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nv\"\u003e--comment-content-alignment\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kc\"\u003eflex-start\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e 8\u003c/span\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nv\"\u003e--font-size-comment-body\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"kt\"\u003erem\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e 9\u003c/span\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e10\u003c/span\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e11\u003c/span\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nc\"\u003ecomment-section\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e12\u003c/span\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ebackground-color\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nf\"\u003evar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e--\u003c/span\u003e\u003cspan class=\"n\"\u003edark\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"kc\"\u003ebrown\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003ealt\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e13\u003c/span\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003epadding\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"kt\"\u003epx\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e14\u003c/span\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eborder-radius\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"kt\"\u003epx\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e15\u003c/span\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e16\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e\u0026lt;/\u003c/span\u003e\u003cspan class=\"nt\"\u003estyle\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAll that was left to take advantage of it was creating a new post (on the blog and on Bluesky)!\u003c/p\u003e\n\u003ch2\u003eClosing Thoughts\u003c/h2\u003e\n\u003cp\u003eBluesky development feels like the wild west right now with many developers creating innovative new tools and integrations for and with the platform, and I'm enjoying being a part of it. I had a lot of fun developing my library and learning about how the Bluesky API works, and I'm quite happy with the result.\u003c/p\u003e\n\u003cp\u003eIf you use Svelte for your website, and want to integrate a comments section powered by Bluesky, give my library a try (and feel free to make suggestions that you think would improve the experience).\u003cbr /\u003e\nIf you are not a Svelte user, but you want to add Bluesky comments, I also recommend Cory Zue's \u003ccode\u003ebluesky-comments\u003c/code\u003e library which is available through NPM\u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e, and I think it can be used with any framework now as well (although the bundle size is considerably bigger than the Svelte version if that is something you care about).\u003c/p\u003e\n\u003cp\u003eOverall, this was a great learning experience, and I look forward to seeing all the cool stuff that gets built around Bluesky.\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr /\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eBluesky appplication repository - \u003ca href=\"https://github.com/bluesky-social/social-app\"\u003ehttps://github.com/bluesky-social/social-app\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eAT Protocol repository - \u003ca href=\"https://github.com/bluesky-social/atproto\"\u003ehttps://github.com/bluesky-social/atproto\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eEmily Liu's comment code - \u003ca href=\"https://gist.github.com/emilyliu7321/19ac4e111588bdc0cb4e411c88d9c79a\"\u003ehttps://gist.github.com/emilyliu7321/19ac4e111588bdc0cb4e411c88d9c79a\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003eCory Zue's comment library - \u003ca href=\"https://bsky.app/profile/coryzue.com/post/3lbrkypd37224\"\u003ehttps://bsky.app/profile/coryzue.com/post/3lbrkypd37224\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003ebluesky-comments-svelte NPM package - \u003ca href=\"https://www.npmjs.com/package/bluesky-comments-svelte\"\u003ehttps://www.npmjs.com/package/bluesky-comments-svelte\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003eMy project repository - \u003ca href=\"https://github.com/ptdewey/bluesky-comments-svelte\"\u003ehttps://github.com/ptdewey/bluesky-comments-svelte\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:7\"\u003e\n\u003cp\u003eCory Zue's library (NPM) - \u003ca href=\"https://www.npmjs.com/package/bluesky-comments\"\u003ehttps://www.npmjs.com/package/bluesky-comments\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n"
  },
  {
    "metadata": {
      "authors": [
        "Patrick Dewey"
      ],
      "categories": [
        "Software Development"
      ],
      "date": "2024-11-10",
      "read_time": 3,
      "slug": "i-rewrote-my-site-in-svelte",
      "tags": [
        "go",
        "swe",
        "software-engineering",
        "software-development",
        "web-development",
        "svelte"
      ],
      "title": "I rewrote my website in Svelte (and Go) and all I got was this lousy blog post"
    },
    "content": "\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI have been using Hugo to run my personal website for a few years now, and it certainly did the job for me, but I was never truly happy with the aesthetics as I wanted something a bit more unique to me.\u003cbr /\u003e\nI could have just made my own theme, but Hugo's template system felt too complicated for what I actually wanted to do, so I decided I would start from scratch using a different tool.\u003cbr /\u003e\nUnfortunately, I do happen to be a fairly busy person (working and grad school and all), so I haven't had enough time to really jump in and build an entire site until this weekend when I had a bit of free time and a sudden (inexplicable) urge to try out Svelte.\u003c/p\u003e\n\u003ch2\u003eGoals\u003c/h2\u003e\n\u003cp\u003eI had a few goals/requirements that I wanted to meet before deploying the new site.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe site styling should be easy to customize and add to (pretty much just styling with CSS)\u003c/li\u003e\n\u003cli\u003eI should be able to generate site content from markdown files (or a similar format like svx)\u003c/li\u003e\n\u003cli\u003eRouting for pages generated from markdown files should be automatic or very simple\u003c/li\u003e\n\u003cli\u003eThe site should be styled based on my \u003ca href=\"https://github.com/ptdewey/darkearth-nvim\"\u003eDarkEarth\u003c/a\u003e color scheme (because its awesome)\u003c/li\u003e\n\u003cli\u003eThe site needs to be static and deployable on GitHub pages\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eThe Rewrite\u003c/h2\u003e\n\u003cp\u003eI started the rewrite by writing some CSS for the project, choosing the colors (based on DarkEarth) and designing a simple navbar.\u003c/p\u003e\n\u003cp\u003eAfter this, I decided to build a small Go program that would convert Markdown files into HTML. I know there are JavaScript libraries I could have used for this, but I simply felt like writing some Go code (that's the only reason).\u003cbr /\u003e\nThe Go program converts all Markdown files found in the \u003ccode\u003econtent\u003c/code\u003e directory into HTML files in \u003ccode\u003estatic\u003c/code\u003e and is built with the \u0026quot;goldmark\u0026quot; library.\u003c/p\u003e\n\u003cp\u003eAfter this, I created a Svelte project, and spent some time learning how the page system works.\u003cbr /\u003e\nAt first, the \u003ccode\u003e+page.svelte\u003c/code\u003e and \u003ccode\u003e+page.js\u003c/code\u003e filenames felt a bit strange, but after playing with Svelte a little bit, I really enjoyed how easy it was to create new pages and style them.\u003cbr /\u003e\nI particularly liked the layout system, where you create a \u003ccode\u003e+layout.svelte\u003c/code\u003e file, which is then used in all pages lower in the file hierarchy.\u003cbr /\u003e\nThis made it super easy for me to set up my navbar and load global CSS in a single place, without having to worry about adding them anywhere else.\u003c/p\u003e\n\u003cp\u003eIt took me a few hours to get everything the way I envisioned, and I also learned about MDsveX, a library that creates a markdown variant that allows the use of Svelte components in it.\u003cbr /\u003e\nI decided to give it a try in a couple of my pages (that were previously in Markdown), and it worked quite well with only a little bit of added configuration.\u003c/p\u003e\n\u003ch2\u003eClosing Thoughts\u003c/h2\u003e\n\u003cp\u003eThe entire rewrite didn't take me nearly as long as I thought it would have, which is certainly somewhat due to Svelte's ease of use and simple syntax.\u003cbr /\u003e\nThe GitHub pages deployment even worked on the first try!\u003c/p\u003e\n\u003cp\u003eOverall, I'm really happy with how the site turned out, and I plan to sporadically improve it going forward (it is much easier to customize now).\u003cbr /\u003e\nI also really enjoyed working with Svelte, which is something I've never said about a frontend framework before, and as such, I highly recommend it if you are looking for a new web framework to try (for some reason).\u003c/p\u003e\n\u003cp\u003eThe code for this project can be found on GitHub \u003ca href=\"https://github.com/ptdewey/ptdewey.github.io\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n"
  },
  {
    "metadata": {
      "authors": [
        "Patrick Dewey"
      ],
      "categories": [
        "Statistics"
      ],
      "date": "2024-05-21",
      "read_time": 9,
      "slug": "signal-and-the-noise",
      "tags": [
        "statistics",
        "bayesian",
        "bayesian-statistics",
        "predictive-analytics",
        "book-review"
      ],
      "title": "A Review of \"The Signal and the Noise\""
    },
    "content": "\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI'm a bit late to the party when it comes to commentary on Nate Silver's book \u003cem\u003ethe Signal and the Noise\u003c/em\u003e\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e, but I recently finished reading it and I have a lot of thoughts about it.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/signal_and_noise_plot.png\" alt=\"\" /\u003e\u003c/p\u003e\n\u003ch2\u003eMain Ideas\u003c/h2\u003e\n\u003cp\u003eFirst, I want to say that I think it is an amazing book that is worth reading for anyone who is interested in statistics, practitioners and laymen alike. If you are considering reading it (and are reading my blog for some reason), I highly recommend it.\u003cbr /\u003e\nThe main goal of the book is to investigate why \u0026quot;a vast majority of predictions fail\u0026quot; and to provide some tips to make better predictions.\u003c/p\u003e\n\u003cp\u003eIn the book, Silver discusses various case studies where predictions either fail or succeed, and explains the reasons behind these outcomes. This deep dive into each of the several subject areas he looked at focused on aspects of their systems that made them difficult (and sometimes impossible) to predict reliably using models, as well as the approaches that the model creators took when trying to generate predictions.\u003cbr /\u003e\nIn his analysis, he repeatedly highlights a tendency for modelers to mistake noise (meaningless patterns present in the data) for signals (meaningful information we seek to extract and understand), leading to very poor predictive performance.\u003cbr /\u003e\nThis tendency was especially common in fields where data is particularly noisy, or where signals are formed by extremely complex interactions between many different variables. In particular, he highlighted earthquake and stock market modeling as examples in which countless people have tried (and failed) to create reliable predictions about the future.\u003c/p\u003e\n\u003cp\u003eAnother important observation Silver made was that a common cause for bad predictions is the tendency for people to make predictions in accordance with the \u0026quot;herd mindset\u0026quot;. In cases like making predictions about the stock market or economy in particular, this tendency is very common, and can often result in bad predictions when the herd tries to be too safe or is too slow to adapt to broader environmental changes. When predicting economical swings, adhering to the herd has few downsides, whereas veering away can have dire consequences to someone's livelihood, especially if their prediction is wrong. Silver notes that this is because staying in the herd when predictions are wrong means everyone is wrong, whereas leaving the herd leaves you exposed without a herd to hide behind. While this can sometimes be good at preventing mass panics, this also has the unfortunate effect of disincentivizing astute analysts from showing off their findings, on the off chance they are wrong.\u003c/p\u003e\n\u003cp\u003eHe also highlighted cases where reliable models for prediction have been established--namely in baseball and weather forecasting--by finding ways to overcome the noise.\u003cbr /\u003e\nI found these chapters to be especially interesting, as both fields involved a vast set of different challenges that had to be overcome, with both fields eventually overcoming these challenges and producing impressive predictions (that we often take for granted in the case of the forecasts).\u003cbr /\u003e\nThe story of how predictions finally started succeeding in weather forecasting stuck out to me as it is a problem that humans were trying to solve for hundreds (or thousands) of years, and we were only able to reliably do it in the last 70 through the convergence of many different fields (meteorology, physics, mathematics, statistics, and computing).\u003c/p\u003e\n\u003cp\u003eIn the back half of the book, he provides suggestions for how to make better predictions, passionately advocating for a much wider adoption of Bayesian statistical methodologies instead of traditionally used frequentist methods. This advocacy for Bayesian reasoning and inference is the main driving point throughout the book.\u003c/p\u003e\n\u003ch2\u003eBayesian Reasoning\u003c/h2\u003e\n\u003cp\u003eBayesian reasoning is a powerful, practical method of translating our beliefs about the world into actionable predictions, in a way that updates our understanding as new information becomes available. By combining prior knowledge with new evidence, Bayesian reasoning allows us to make more informed decisions, continuously refining our predictions to better reflect reality. This approach is very similar to the scientific method, where hypotheses are tested and updated based on experimental data. Both rely on an iterative process of forming expectations, gathering data, and revising beliefs, providing a systematic and intuitive framework for dealing with uncertainty\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eRunning parallel to the Bayesian methodology is the alternative methodological school of statistics, frequentism. Frequentist statistics is focused on long run frequencies for(hypothetical) probabilistic events, running in contrast to the Bayesian school which is focused more on single-event probabilities and the process of using and updating beliefs. Both methodologies have their merits, with both having their own strengths, but both I and Nate Silver consider ourselves Bayesians (although I land much closer to the middle than he presents himself).\u003c/p\u003e\n\u003cp\u003eIn the book, Silver takes the stance that applying Bayesian reasoning to the different situations he talks about would lead to better predictions that are reliant on fewer assumptions and are less likely to mistake noise for signal. He also posits that many success stories in the world of prediction result from the application of Bayesian thinking (whether that is intentional or simply through intuition).\u003c/p\u003e\n\u003cp\u003eI would like to elaborate further on Bayesian reasoning, inference, and how it fits into the realm of machine learning in another post in the near future.\u003c/p\u003e\n\u003ch2\u003eCriticisms\u003c/h2\u003e\n\u003cp\u003eMy main point on contention with Silver's point of view is the way that he positions the Bayesian methodology as wholly better than the frequentist alternative. While I would consider myself a Bayesian, I also understand that frequentist methods have their uses, particularly in cases where computational power is a limiting factor (Bayesian-based methods tend to be considerably more computationally intensive).\u003c/p\u003e\n\u003cp\u003eI take issue with the way he asserts that we should entirely replace frequentist methods with Bayesian alternatives as a way of dealing with issues inherent to the frequentist approach (misinterpreting p-values, p-values close to the cutoff, misinterpreting confidence intervals, etc.). I don't disagree that these are very real issues that come up quite often (and often lead to bad science), and that Bayesian methods \u003cem\u003ecould\u003c/em\u003e solve these issues. Unfortunately, the Bayesian approach is not without its own issues. The main criticism that can be brought against the methodology revolves around choice of the prior distribution, which can be viewed as overly arbitrary (and can lead to bad science when disingenuous choices are made). Since the Bayesian methodology is entirely focused on the process of using and updating beliefs, a choice of a very strong prior distribution can result in very inaccurate (unrepresentative or potentially even intentionally cherry-picked) results. On top of all this, frequentist and Bayesian methods are not even entirely mutually exclusive, and knowing when to use each can be a valuable skill in the field, regardless of which methodology you prefer (especially in machine learning contexts).\u003c/p\u003e\n\u003cp\u003eSilver also states that the only bad choice of prior belief is no belief. I wholly disagree with this as priors can be overly strong (as previously mentioned), or can be based on little to no evidence. In cases lacking adequate data--whether as a result of personal experience or other reasons--a choice of an \u003cem\u003euninformative\u003c/em\u003e prior would be the correct choice. Uninformative priors are types of prior distribution that are used in cases where we don't want to use any of our subjective beliefs and we want to make our Bayesian inferences as objectively as possible\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eIn one of the last case studies of the book, Silver discusses issues with climate modeling, as well as skepticism from experts and non-experts that result from models or their methodologies. This section includes a part where he compares models from two different sources, the IPCC (Intergovernmental Panel on Climate Change), and J. Scott Armstrong from the Heartland Institute (a conservative libertarian think tank). In this section of the book, Silver interviews Armstrong about his no-change model and his subsequent attempt to make a bet with Al Gore about the veracity of the IPCC model\u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e. At the time (2013), Armstrong's no change model (2009) was performing better than the IPCC's climate model (2007)\u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e at predicting yearly change in global temperatures. The part of this I take issue with is how Silver heaps criticism onto the IPCC model about how it is overly complicated (which is partially true) and could be wrong, without applying any similar criticisms to Armstrong's model--a model produced by a conservative think tank with a history of producing dubious (and outright false) claims\u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e. Silver didn't even raise the obvious criticism that the \u0026quot;no-change\u0026quot; model chose a baseline global temperature of the hottest year on record at the time, 1998\u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/temperature-change-plot.png\" alt=\"\" /\u003e\u003c/p\u003e\n\u003cp\u003eI would consider this baseline choice as particularly egregious as (with the full context of the above graph), it ignores a century of increasing global temperatures in order to serve a political narrative. This stuck out to me, partially as a young person who cares a lot about climate change, but also because I read the previous sections of this exact book, in which Silver calls out pundits for making predictions that serve their narrative, and are often incorrect as a result. I don't believe that Nate Silver is a climate denier by any means, but I do think it was harmful that he perpetuated the climate skepticism narrative by not applying adequate scrutiny to Armstrong's model.\u003cbr /\u003e\nI do acknowledge that it has been more than a decade since this book was written, but in hindsight this section has aged quite poorly. As of 2024, we have had the three hottest years on record back to back, and the annual global temperature changes are much more in line with the IPCC than with Armstrong's no change model (which looks pretty bad now\u003csup id=\"fnref1:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e). As climate change continues to grow worse, writings like this section of the book are damaging to momentum towards dealing with the issue.\u003c/p\u003e\n\u003ch2\u003eFinal Thoughts\u003c/h2\u003e\n\u003cp\u003eAs I said before, I really enjoyed this book and the case studies it investigates. I found the explanation of the Bayesian methodology and how it meshes well with human intuition very interesting, as well as how Silver seeks to apply it to create better predictions. Other than a couple of notable critiques, my thoughts on the book are entirely positive, and I would highly recommend it to anyone interested in statistics. My beliefs about frequentist and Bayesian statistics shifted a bit more towards the Bayesian side in light of the new data (this book).\u003cbr /\u003e\nAt the end of it all, I found this book to be a very interesting read, and I will continue to follow Silver's work\u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e of modeling elections and more.\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr /\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eThe Signal and The Noise - \u003ca href=\"https://www.penguinrandomhouse.com/books/305826/the-signal-and-the-noise-by-nate-silver/\"\u003ehttps://www.penguinrandomhouse.com/books/305826/the-signal-and-the-noise-by-nate-silver/\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eBayes Rules! An Introduction to Applied Bayesian Modeling - \u003ca href=\"https://www.bayesrulesbook.com\"\u003ehttps://www.bayesrulesbook.com\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eUninformative priors - \u003ca href=\"https://www.statlect.com/fundamentals-of-statistics/uninformative-prior\"\u003ehttps://www.statlect.com/fundamentals-of-statistics/uninformative-prior\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003e\u003ca href=\"https://www.researchgate.net/publication/349237629_The_Global_Warming_Challenge_Evidence-based_forecasting_for_climate_change_theclimatebetcom#fullTextFileContent\"\u003ehttps://www.researchgate.net/publication/349237629_The_Global_Warming_Challenge_Evidence-based_forecasting_for_climate_change_theclimatebetcom#fullTextFileContent\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref1:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003eIPCC report - \u003ca href=\"https://www.ipcc.ch/site/assets/uploads/2018/02/ar4_syr_full_report.pdf\"\u003ehttps://www.ipcc.ch/site/assets/uploads/2018/02/ar4_syr_full_report.pdf\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003e\u003ca href=\"https://www.politifact.com/personalities/heartland-institute/\"\u003ehttps://www.politifact.com/personalities/heartland-institute/\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:7\"\u003e\n\u003cp\u003eChange in global average temperature - \u003ca href=\"https://berkeleyearth.org/global-temperature-report-for-2023/\"\u003ehttps://berkeleyearth.org/global-temperature-report-for-2023/\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:8\"\u003e\n\u003cp\u003eNate Silver's \u003cem\u003eSilver Bulletin\u003c/em\u003e Blog - \u003ca href=\"https://www.natesilver.net/\"\u003ehttps://www.natesilver.net/\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:8\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n"
  },
  {
    "metadata": {
      "authors": [
        "Patrick Dewey"
      ],
      "categories": [
        "Data Science",
        "Programming"
      ],
      "date": "2024-05-13",
      "read_time": 7,
      "slug": "neural-net-viz",
      "tags": [
        "machine-learning",
        "ai",
        "python",
        "r",
        "visualization"
      ],
      "title": "Visually Representing *What* Neural Networks Learn"
    },
    "content": "\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eRecently, I just finished a semester-long project in which a colleague and I investigated how neural networks learn using visualizations. As a computer and data scientist, this topic has fascinated me for a while. We spend so much time constructing and refining neural networks, but very little time actually trying to understand how they work behind the scenes.\u003c/p\u003e\n\u003ch2\u003eBackground\u003c/h2\u003e\n\u003cp\u003eNeural networks are inherently black-box models, meaning data goes in one side, and results come out the other. This is unfortunate, as it means that neither the developer nor the end user has any idea what is actually taking place within the network. This results in outputs that are often uninterpretable and can call into question whether or not the results are even trustworthy.\u003c/p\u003e\n\u003cp\u003eThis is a major problem, and it is only getting worse as we grow increasingly reliant on AI models in every aspect of our lives. It grows even worse when we take into account the fact that our models are becoming bigger and more complex, with the latest fascination being large language models (LLMs).\u003c/p\u003e\n\u003cp\u003eUnfortunately, there is no simple solution to this issue, and even beginning to look into the learning process within a network requires an engaged effort from the creator. Our project began an investigation into methods through which neural networks could be visualized, with the goal of adding transparency to the model training process and turning AI into XAI (Explainable AI).\u003c/p\u003e\n\u003ch3\u003eProject Scope\u003c/h3\u003e\n\u003cp\u003eAs this was a semester project, the scope had to be fairly limited. Most notably, we did not set out to try and set up a modular application in which any network could be visualized and instead focused on a few chosen datasets (to establish a prototype of sorts).\u003c/p\u003e\n\u003cp\u003eWe also only looked at classification networks, as regression networks are another animal entirely and would likely suffer from even more extreme interpretability issues than classifiers\u003c/p\u003e\n\u003ch2\u003eDesign Objectives and Decisions\u003c/h2\u003e\n\u003cp\u003eOur approach to visualizing the learning process consisted of visualizing network weights (for image classification tasks) through the use of animated image plots. To create these plots, we plotted each weight in a layer as a single pixel in the image, with the color representing the value of that weight. Each image is made up of all the weights for a single neuron in a layer, and the images were arranged in a grid with all the images for that layer. To show how these weights evolve over the course of network training, we stitched the grid plots together into an animation.\u003c/p\u003e\n\u003cp\u003eThe other main component of our visualization of the learning process was visualizing network decision boundaries over time. To do this, we created a scatterplot of the data points and overlaid it on top of a contour plot representing the decision boundary of the network. In the scatterplot, the color of a point represents the class it is a part of. The colors of the contours are a bit harder to interpret (an area for improvement), but the dark red regions represent the actual decision boundary, and the network becomes more uncertain (represented as the color getting lighter, then changing to blue) as it gets farther away from the dark red region. Again, we wanted to show how the decision boundary changes during the training process, so we stitched the contour plots together into an animation.\u003c/p\u003e\n\u003ch3\u003eTools Used\u003c/h3\u003e\n\u003cp\u003eTo train our network and store the weights, we used Python with the PyTorch and SciKit-Learn libraries.\u003cbr /\u003e\nThe weight and boundary visualizations were created in Python and animated using ffmpeg.\u003cbr /\u003e\nThe exploratory PCA and \u003cem\u003et\u003c/em\u003e-SNE plots were created with R and ggplot, with the 3-dimensional plots being made with ggplotly.\u003cbr /\u003e\nThe overarching website was created using R-markdown, knit (compiled) to a couple of html pages, and then hosted using GitHub pages.\u003c/p\u003e\n\u003ch2\u003eResults\u003c/h2\u003e\n\u003cp\u003eThe website for this project can be found at \u003ca href=\"https://pdewey.com/neural-net-viz\"\u003ehttps://pdewey.com/neural-net-viz\u003c/a\u003e \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. The source code for our code can also be found below \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003ch3\u003eVisualizing Decision Boundaries\u003c/h3\u003e\n\u003cp\u003eThe decision boundary contour plot animations demonstrate how a neural network's decision boundary evolves and bisects the dataset it is trained on. Points represent the dataset, and the contoured background shows the decision boundary, with colors indicating the distance from the boundary. For example, using the Moons dataset, a highly nonlinear boundary is needed for correct classification.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/nnviz-db-e1.png\" alt=\"\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDecision Boundary: Epoch 1\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe initial state of the decision boundary is completely random at the start of training.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/nnviz-db-e20.png\" alt=\"\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDecision Boundary: Epoch 20\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAfter 20 epochs, the boundary has shaped to separate many of the points from each cluster, but it still misclassifies many entries at this point.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/nnviz-db-e100.png\" alt=\"\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDecision Boundary: Epoch 100\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAfter 100 epochs, we can see that the decision boundary perfectly separates the two classes.\u003c/p\u003e\n\u003ch3\u003eDimensionality Reduction for High-Dimensional Datasets\u003c/h3\u003e\n\u003cp\u003eThe exploratory analysis page includes plots using Principal Component Analysis (PCA) and \u003cem\u003et\u003c/em\u003e-Distributed Stochastic Neighbor Embedding (\u003cem\u003et\u003c/em\u003e-SNE) to illustrate class distributions in the MNIST dataset. PCA plots show blurred boundaries between classes, while \u003cem\u003et\u003c/em\u003e-SNE plots reveal distinct clusters and greater sensitivity to in-group differences. For both PCA and \u003cem\u003et\u003c/em\u003e-SNE we included a biplot, de\u003cbr /\u003e\nnsity biplot, and 3D plot of the class embeddings.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/nnviz-pca-biplots.png\" alt=\"\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePCA Biplot and Density Biplot\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/nnviz-tsne-biplots.png\" alt=\"\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003et\u003c/em\u003e-SNE Biplot and Density Biplot\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBetween the two dimensionality reduction algorithms, \u003cem\u003et\u003c/em\u003e-SNE was better in this case at visually separating the classes from the dataset. We found the density biplot to be the most effective at visually communicating the distribution of the different data classes.\u003c/p\u003e\n\u003ch3\u003eVisualizing Neuron Weights\u003c/h3\u003e\n\u003cp\u003eWeight visualization animations show how neuron weights evolve during training for high-dimensional datasets like MNIST. Initially random, many of these weights gradually form (somewhat) recognizable patterns.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/nnviz-wv-e1.png\" alt=\"\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNeuron Weights: Epoch 1\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/nnviz-wv-e20.png\" alt=\"\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNeuron Weights: Epoch 20\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAfter 20 epochs, we can see a number of patterns that resemble entire numbers or parts of them.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/nnviz-wv-e100.png\" alt=\"\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNeuron Weights: Epoch 100\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAfter 100 epochs, most weights became very muddled, still holding the previous patterns but in more muddy state. This may seem counterintuitive as the network appears (from the eyes of a human) to have lost the original patterns, but the network was performing noticeably better in classification tasks at the 100 epoch mark.\u003c/p\u003e\n\u003ch2\u003eLimitations\u003c/h2\u003e\n\u003cp\u003eA significant challenge in this project was visualizing high-dimensional data, decision boundaries, and weights. For instance, it wasn't possible to plot the decision boundary for the MNIST image classification dataset in a contour plot due to its high dimensionality. However, by using weight visualizations, it was possible to represent individual components of the decision boundary, offering insights into the network's behavior when trained on this dataset.\u003cbr /\u003e\nTo address the dimensionality issue, PCA could be used to shrink the decision boundary into two or three dimensions, making visualization possible through contour plots. While this approach might produce messy or unclear results, it warrants further exploration and could yield intriguing findings.\u003c/p\u003e\n\u003cp\u003eAnother inherent limitation is that for large datasets with many unique output classes, weight visualizations often become unintelligible due to the wide range of possible weight values. This issue is more complex than the dimensionality problem but represents another potential area for future research.\u003c/p\u003e\n\u003ch2\u003eClosing Thoughts\u003c/h2\u003e\n\u003cp\u003eThis project underscored the complexity of visualizing neural networks, especially when dealing with high-dimensional data. Despite the challenges, our efforts provided valuable insights into the learning processes of neural networks. By visualizing decision boundaries and neuron weights, we were able to peek into the black-box nature of these models and understand their behavior better.\u003c/p\u003e\n\u003cp\u003eThe techniques we used, like PCA and \u003cem\u003et\u003c/em\u003e-SNE for dimensionality reduction, and animated weight visualizations, showed promise in making neural networks more interpretable. However, there are still significant limitations, especially in visualizing high-dimensional decision boundaries and weights for large datasets with many classes.\u003c/p\u003e\n\u003cp\u003eFuture research could explore more advanced dimensionality reduction techniques or develop new methods for visualizing neural networks. Our project is a step towards making AI models more transparent and trustworthy, contributing to the broader goal of Explainable AI (XAI). By continuing to innovate and experiment, we can hope to further demystify these powerful yet complex models.\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr /\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003e\u003ca href=\"https://pdewey.com/neural-net-viz/\"\u003eProject Site\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ptdewey/neural-net-viz/\"\u003eSource Code\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n"
  }
]